

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="shuaijiang">
  <meta name="keywords" content="">
  
    <meta name="description" content="大模型时代的 Audio Tokenizer 总结与思考写在最前面：访问我的知乎同款文章 背景大语言模型拉开了大模型时代的序幕。大语言模型的核心机制是对文本序列建模：通过预测下一个词或字，实现对语言结构和语义的理解与生成。由于文字已经是高度抽象的、符号化的表达，适合直接建模。 随着大模型能力的拓展，多模态融合逐渐成为研究热点。在众多模态中，音频是最重要、最常见的模态之一，广泛出现在语音识别、合成、">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型时代的Audio Tokenizer总结与思考">
<meta property="og:url" content="http://zhaoshuaijiang.com/2025/07/25/audio-codec/index.html">
<meta property="og:site_name" content="shuaijiang&#39;s blog">
<meta property="og:description" content="大模型时代的 Audio Tokenizer 总结与思考写在最前面：访问我的知乎同款文章 背景大语言模型拉开了大模型时代的序幕。大语言模型的核心机制是对文本序列建模：通过预测下一个词或字，实现对语言结构和语义的理解与生成。由于文字已经是高度抽象的、符号化的表达，适合直接建模。 随着大模型能力的拓展，多模态融合逐渐成为研究热点。在众多模态中，音频是最重要、最常见的模态之一，广泛出现在语音识别、合成、">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_discrete.png">
<meta property="og:image" content="https://img.shields.io/badge/arXiv-2107.03312-b31b1b.svg?logo=arXiv">
<meta property="og:image" content="https://img.shields.io/badge/Github-Unofficial-lucidrains/vector--quantize--pytorch.svg?logo=github">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_soundstream.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_soundstream_rvq.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_soundstream_result.png">
<meta property="og:image" content="https://img.shields.io/badge/arXiv-2210.13441-b31b1b.svg?logo=arXiv">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-facebookresearch/encodec.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-EnCodec-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/arXiv-2405.00233-b31b1b.svg?logo=arXiv">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-haoheliu/SemantiCodec--inference.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%93%BA%20Demo-SemantiCodec-blue.svg">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_semantic_codec_rate.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_semantic_codec.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_semantic_codec_result.png">
<meta property="og:image" content="https://img.shields.io/badge/arXiv-2408.16532-b31b1b.svg?logo=arXiv">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-jishengpeng/WavTokenizer.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-WavTokenizer-blue.svg">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_wavtokenizer_result.png">
<meta property="og:image" content="https://img.shields.io/badge/arXiv-2411.19842-b31b1b.svg?logo=arXiv">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-Stability--AI/stable--codec.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-stable--codec--speech--16k-blue.svg">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_stable_codec.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_fsq.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_stable_codec_mushra.png">
<meta property="og:image" content="https://img.shields.io/badge/Blog-MOSS--TTSD-blue.svg?logo=googlechrome">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-OpenMOSS/MOSS--TTSD.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-XY__Tokenizer__TTSD__V0-blue.svg">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_xy_tokenizer.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_xy_tokenizer_result.png">
<meta property="og:image" content="https://img.shields.io/badge/Blog-Github-blue.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/Github-Code-boson--ai/higgs--audio.svg?logo=github">
<meta property="og:image" content="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-higgs--audio--v2--tokenizer-blue.svg">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_higgs_audio_tokenizer.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_higgs_audio_tokenizer_semantic.png">
<meta property="og:image" content="http://zhaoshuaijiang.com/image/audiocodec_higgs_audio_tokenizer_acoustic.png">
<meta property="article:published_time" content="2025-07-25T03:58:30.000Z">
<meta property="article:modified_time" content="2025-07-31T03:42:42.073Z">
<meta property="article:author" content="shuaijiang">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="Tokenizer">
<meta property="article:tag" content="audio codec">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://zhaoshuaijiang.com/image/audiocodec_discrete.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>大模型时代的Audio Tokenizer总结与思考 - shuaijiang&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"zhaoshuaijiang.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  <style>ins.adsbygoogle[data-ad-status="unfilled"] { display: none !important; }</style>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>shuaijiang&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="大模型时代的Audio Tokenizer总结与思考"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-25 11:58" pubdate>
          2025年7月25日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          36 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
<aside class="sidebar d-none d-xl-block" style="margin-right:-1rem;z-index:-1"><ins class="adsbygoogle" style="display:flex;justify-content:center;min-width:160px;max-width:300px;width:100%;height:600px;position:sticky;top:2rem" data-ad-client="ca-pub-3762972897277715" data-ad-slot="3762972897277715"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({}); </script></aside>
    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">大模型时代的Audio Tokenizer总结与思考</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="大模型时代的-Audio-Tokenizer-总结与思考"><a href="#大模型时代的-Audio-Tokenizer-总结与思考" class="headerlink" title="大模型时代的 Audio Tokenizer 总结与思考"></a>大模型时代的 Audio Tokenizer 总结与思考</h1><p>写在最前面：访问<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1931648584375341758">我的知乎同款文章</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>大语言模型拉开了大模型时代的序幕。大语言模型的核心机制是对文本序列建模：通过预测下一个词或字，实现对语言结构和语义的理解与生成。由于文字已经是高度抽象的、符号化的表达，适合直接建模。</p>
<p>随着大模型能力的拓展，多模态融合逐渐成为研究热点。在众多模态中，<strong>音频是最重要、最常见的模态之一</strong>，广泛出现在语音识别、合成、声纹、增强、音频检索、人机交互等应用中。</p>
<p>然而，相较于文本，音频具有以下特点：</p>
<ul>
<li><strong>连续性强</strong></li>
<li><strong>冗余度高</strong></li>
<li><strong>信息密度不均衡</strong></li>
</ul>
<p>这些特性使得音频难以直接建模，需要转化为系统或模型所能接受的表示形式，同时降低冗余信息并保留有用信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">思考问题1：声音是一种波，计算机中用什么方式存储？注：语音是连续信号，一般经数字化表示，电话8KHz采样率（即每秒8000个采样点，人声频率范围一般小于1KHz，8KHz满足香农采样定理要求），普通音质16KHz，高音质24KHz、48KHz甚至更高。<br></code></pre></td></tr></table></figure>

<p>在过去的几十年中，音频的表示形式经历了从人工特征工程到深度学习特征表示再到Neural Audio Codec的演进：</p>
<ul>
<li><strong>传统特征工程</strong>：如 MFCC、FBank 等，借鉴人类听觉机制设计，具有实现简单、计算高效等优势，但表达能力有限。</li>
<li><strong>深度学习表示</strong>：如自编码器（AE）、变分自编码器（VAE）的 latent 向量，或是基于 Transformer 的 HuBERT、Wav2Vec 等模型，在捕捉语音中的高层语义结构上表现更强。</li>
<li><strong>Neural Audio Codec</strong>：最近成为主流方向。这类方法将音频压缩为离散 token 序列，不仅方便大模型以自回归方式建模，还支持语音的高质量重建，使得音频的理解与生成任务可以统一建模。</li>
</ul>
<p><img src="/image/audiocodec_discrete.png" srcset="/img/loading.gif" lazyload alt="音频离散token化(https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html)"></p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">思考问题<span class="hljs-number">2</span>：大模型建模如何融合文本和语音？一般人说话过程中平均每秒说几个字？哪种音频表示方法更适合大模型？注：每秒语音有<span class="hljs-number">16000</span>个采样点（以常见<span class="hljs-number">16</span>KHz语音为例），传统特征每秒<span class="hljs-number">40</span>~<span class="hljs-number">100</span>帧连续特征，Hubert等网络表示特征每秒<span class="hljs-number">50</span>帧连续特征，Neural <span class="hljs-built_in">Audio</span> Codec每秒<span class="hljs-number">26</span>~<span class="hljs-number">1000</span> 离散Token。<br></code></pre></td></tr></table></figure>

<hr>
<h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>对目前出现的 Audio Tokenizer 进行整理，并详细列出码本数、码率等特性，便于对比分析。</p>
<table>
<thead>
<tr>
<th>Tokenizer</th>
<th>时间 &amp; 作者</th>
<th>码本数</th>
<th>Token Rate (Hz)</th>
<th>Bit Rate (kbps)</th>
<th>采样率 (kHz)</th>
<th>训练数据</th>
<th>应用</th>
</tr>
</thead>
<tbody><tr>
<td>SoundStream</td>
<td>2021.07 Google</td>
<td>8~80</td>
<td>75×码本数</td>
<td>3~18</td>
<td>24</td>
<td>LibriTTS、音乐</td>
<td>AudioLM</td>
</tr>
<tr>
<td>Encodec</td>
<td>2022.10 Facebook</td>
<td>2,4,8,16,32</td>
<td>75×码本数 / 150×码本数</td>
<td>1.5,3,6,12,24 / 3,6,12,24</td>
<td>24 / 48</td>
<td>Common-Voice, DAPS, Jamendo, AudioSet, FSD50K</td>
<td>VALLE</td>
</tr>
<tr>
<td>AudioDec</td>
<td>2023.05 Meta</td>
<td>8</td>
<td>1280</td>
<td>12.8</td>
<td>48</td>
<td>Valentini</td>
<td>-</td>
</tr>
<tr>
<td>AcademiCodec</td>
<td>2023.05 北大 &amp; 腾讯</td>
<td>4</td>
<td>-</td>
<td>2,3</td>
<td>16</td>
<td>LibriTTS, VCTK, AISHELL</td>
<td>-</td>
</tr>
<tr>
<td>DAC</td>
<td>2023.06 Descript</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SpeechTokenizer</td>
<td>2023.08 复旦</td>
<td>8</td>
<td>-</td>
<td>4</td>
<td>16</td>
<td>LibriSpeech</td>
<td>SpeechGPT, AnyGPT</td>
</tr>
<tr>
<td>Funcodec</td>
<td>2023.09 阿里</td>
<td>32</td>
<td>-</td>
<td>16</td>
<td>16</td>
<td>LibriTTS、25K hours others</td>
<td>LauraGPT</td>
</tr>
<tr>
<td>CosyToken</td>
<td>2024.07 阿里</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>16</td>
<td>13wh ZH、3wh EN、5kh Yue、4.6kh JP、2.2kh KO</td>
<td>CosyVoice</td>
</tr>
<tr>
<td>Xcodec</td>
<td>2024.08 港科大 &amp; Microsoft</td>
<td>8</td>
<td>-</td>
<td>4</td>
<td>16</td>
<td>LibriSpeech</td>
<td>-</td>
</tr>
<tr>
<td>WavTokenizer</td>
<td>2024.08 浙大</td>
<td>1</td>
<td>40, 75</td>
<td>0.5, 0.9</td>
<td>24</td>
<td>LibriTTS、VCTK、CommonVoice、LibriLight 共80k小时</td>
<td>-</td>
</tr>
<tr>
<td>SemantiCodec</td>
<td>2024.05 萨里大学 &amp; 上海交大</td>
<td>-</td>
<td>25,50,100</td>
<td>0.31~1.40</td>
<td>16</td>
<td>GigaSpeech, VoiceFixer, Million Song, MedleyDB, MUSDB18, AudioSet</td>
<td>-</td>
</tr>
<tr>
<td>Mimi codec</td>
<td>2024.10 Kyutai</td>
<td>8</td>
<td>-</td>
<td>1.1</td>
<td>24</td>
<td>-</td>
<td>Moshi</td>
</tr>
<tr>
<td>Stable Codec</td>
<td>2024.11 Stability-AI</td>
<td>3</td>
<td>25, 50</td>
<td>0.4, 0.7</td>
<td>16</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Step-Audio-Tokenizer</td>
<td>2025.02 stepfun-ai</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>BiCodec</td>
<td>2025.03 SparkAudio</td>
<td>1</td>
<td>50</td>
<td>0.65</td>
<td>16</td>
<td>LibriSpeech, Emilia-CN, Emilia-EN</td>
<td>Spark-TTS</td>
</tr>
<tr>
<td>ALMTokenizer</td>
<td>2025.04 港中大</td>
<td>3</td>
<td>37.5</td>
<td>0.41</td>
<td>24</td>
<td>LibriTTS, MLS, AudioSet, Million Song</td>
<td>-</td>
</tr>
<tr>
<td>higgs-audio-v2-tokenizer</td>
<td>2025.07 BosonAI</td>
<td>8</td>
<td>25×8</td>
<td>2.0</td>
<td>24</td>
<td>-</td>
<td>higgs-audio</td>
</tr>
</tbody></table>
<hr>
<h2 id="Neural-Audio-Codec-模型详解"><a href="#Neural-Audio-Codec-模型详解" class="headerlink" title="Neural Audio Codec 模型详解"></a>Neural Audio Codec 模型详解</h2><h3 id="SoundStream-2021-07"><a href="#SoundStream-2021-07" class="headerlink" title="SoundStream (2021.07)  "></a>SoundStream (2021.07) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.03312"><img src="https://img.shields.io/badge/arXiv-2107.03312-b31b1b.svg?logo=arXiv" srcset="/img/loading.gif" lazyload alt="arXiv"></a> <a target="_blank" rel="noopener" href="https://github.com/lucidrains/vector-quantize-pytorch"><img src="https://img.shields.io/badge/Github-Unofficial-lucidrains/vector--quantize--pytorch.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a></h3><p>SoundStream 是由 Google Research 的 Neil Zeghidour 等人在 2021 年提出的，可以说是 Neural Audio Codec 方向的开山之作。值得一提的是，Neil Zeghidour 也是 Moshi 的主要作者，在音频建模领域贡献颇多。<br>SoundStream 的提出标志着一种新范式的开始：利用端到端神经网络对音频进行压缩，并生成离散化的 token 表达，以支持下游建模任务，特别适合与大语言模型对接。</p>
<h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>整体架构采用 编码器-量化器-解码器（Encoder-RVQ-Decoder） 的方式，具体来说：</p>
<ul>
<li>Encoder 将原始波形压缩为低维表示；</li>
<li>Residual Vector Quantization（RVQ） 将连续表示离散化为多组 token；分多级量化器，每级负责编码残差信息，逐步逼近原始向量。</li>
<li>Decoder 从离散 token 重建出原始音频。</li>
</ul>
<p><img src="/image/audiocodec_soundstream.png" srcset="/img/loading.gif" lazyload alt="SoundStream模块框架"></p>
<h4 id="RVQ算法"><a href="#RVQ算法" class="headerlink" title="RVQ算法"></a>RVQ算法</h4><p>在 Neural Audio Codec 中，最关键的一步是如何将连续信号离散化为 token 表示。SoundStream 使用了 Residual Vector Quantization（RVQ） 技术，其主要思想是分多级量化器，每级负责编码残差信息，从而逐步逼近原始向量。更多详细解读<br><img src="/image/audiocodec_soundstream_rvq.png" srcset="/img/loading.gif" lazyload alt="RVQ"></p>
<h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>SoundStream 在语音重建质量上优于传统压缩方法（如 Opus），尤其在低比特率下依然保持不错的主观听感。其强大的表示能力也使其适用于后续的语音合成、生成、编辑等任务。<br><img src="/image/audiocodec_soundstream_result.png" srcset="/img/loading.gif" lazyload alt="SoundStream效果"></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>SoundStream 是 Neural Audio Codec 方向的奠基之作，不仅首次系统性地提出神经音频压缩框架，也确立了 RVQ 作为音频离散化的核心技术路径。虽然机制并不复杂，但在实际中非常高效，影响了后续整个音频tokenization领域的发展。</p>
<hr>
<h3 id="EnCodec-2022-10"><a href="#EnCodec-2022-10" class="headerlink" title="EnCodec(2022.10)   "></a>EnCodec(2022.10) <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.13438"><img src="https://img.shields.io/badge/arXiv-2210.13441-b31b1b.svg?logo=arXiv" srcset="/img/loading.gif" lazyload alt="arXiv"></a> <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/encodec"><img src="https://img.shields.io/badge/Github-Code-facebookresearch/encodec.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/encodec"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-EnCodec-blue.svg" srcset="/img/loading.gif" lazyload alt="model"></a></h3><p>EnCodec 是 Meta AI 于 2022 年提出的 Neural Audio Codec 模型，作为对 SoundStream 框架的延续和改进，它同样采用了 Residual Vector Quantization（RVQ） 技术，并在音频质量、适用范围和开源支持方面做了全面升级:</p>
<ul>
<li><strong>多采样率支持</strong>：EnCodec 同时支持 24kHz 和 48kHz 音频信号，覆盖语音和高保真音频等不同应用场景；</li>
<li><strong>多比特率可调</strong>：提供从 1.5 到 24 kbps 的多个比特率选项，适用于不同带宽和重建质量的需求；</li>
<li><strong>延续 RVQ 框架</strong>：与 SoundStream 一样，EnCodec 使用多层残差量化器将音频编码为离散 token 序列，使得大模型可以直接用于建模；</li>
<li><strong>开源可复现性强</strong>：Meta 提供了完整的代码、模型权重与示例脚本，极大地推动了社区在音频离散化方向的研究与落地。</li>
<li><strong>影响力广泛</strong>：微软提出的端到端语音合成模型VALLE，直接采用 EnCodec 生成离散语音 token；Meta 的音乐生成模型MusicGen，也使用 EnCodec 作为音乐 tokenization 的底层模块；</li>
</ul>
<h4 id="模型架构-1"><a href="#模型架构-1" class="headerlink" title="模型架构"></a>模型架构</h4><p>EnCodec 仍采用编码器-量化器-解码器的标准框架。相比 SoundStream，EnCodec 在编码器与解码器的架构上进行了优化，引入了更深层次的卷积模块与归一化技术，提升了压缩后的重建质量。</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p>EnCodec 在保持高压缩效率的同时提供更高的可控性和更好的重建质量，是 Neural Audio Codec 技术的重要里程碑。其高质量开源实现加速了音频 tokenization 技术的工程化应用，也为大模型时代的多模态建模奠定了坚实基础。</p>
<hr>
<h3 id="SemantiCodec-2024-05"><a href="#SemantiCodec-2024-05" class="headerlink" title="SemantiCodec (2024.05)   "></a>SemantiCodec (2024.05) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.00233"><img src="https://img.shields.io/badge/arXiv-2405.00233-b31b1b.svg?logo=arXiv" srcset="/img/loading.gif" lazyload alt="arXiv"></a> <a target="_blank" rel="noopener" href="https://github.com/haoheliu/SemantiCodec-inference"><img src="https://img.shields.io/badge/Github-Code-haoheliu/SemantiCodec--inference.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://haoheliu.github.io/SemantiCodec/"><img src="https://img.shields.io/badge/%F0%9F%93%BA%20Demo-SemantiCodec-blue.svg" srcset="/img/loading.gif" lazyload alt="demo"></a></h3><p>SemantiCodec 是由萨里大学与上海交通大学联合提出的一种极低码率的开源 Audio Tokenizer，专为语义理解与生成任务设计。其核心创新在于显式建模语义与声学特征的联合表示，以支持更强的语义能力与灵活的压缩率选择。<br>强语义建模能力：通过引入 AudioMAE 语义嵌入，提升离散表示的语义表达力；<br>丰富的 TokenRate 与 BitRate 选择：支持 25、50、100 tokens/s 三档 TokenRate，对应比特率范围为 0.31~1.43 Kbps，便于适配不同带宽和任务需求。具体如下图所示：</p>
<p><img src="/image/audiocodec_semantic_codec_rate.png" srcset="/img/loading.gif" lazyload alt="SemantiCodec TokenRate and BitRate"></p>
<h4 id="模型架构-2"><a href="#模型架构-2" class="headerlink" title="模型架构"></a>模型架构</h4><ul>
<li>语义编码：首先从输入音频中提取 AudioMAE 嵌入，并通过 K-Means 聚类量化为语义表示 $E_s$；</li>
<li>声学补全：将原始音频帧 $Y$ 与语义表示 $E_s$ 拼接输入 Residual Encoder，提取细粒度声学信息；</li>
<li>向量量化：上述声学向量经 VQ 模块量化为 $E_a$；</li>
<li>融合表示：最终音频表示 $E$ 由 $E_s$ 与 $E_a$ 拼接构成，用于音频重建；</li>
<li>生成器：使用 Latent Diffusion Model 在条件表示 $E$ 上进行音频生成。</li>
</ul>
<p><img src="/image/audiocodec_semantic_codec.png" srcset="/img/loading.gif" lazyload alt="SemantiCodec模型框架"></p>
<h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p>在主观音质评估基准 MUSHRA 上，SemantiCodec 明显优于 Encodec、Descript 等方法，尤其在低码率条件下表现出更好的听感质量。</p>
<p><img src="/image/audiocodec_semantic_codec_result.png" srcset="/img/loading.gif" lazyload alt="SemantiCodec效果"></p>
<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p>SemantiCodec 是一款支持 极低 BitRate 与 TokenRate 的开源 Audio Codec，具备可调节压缩粒度、显式语义建模、高保真语音重建等特点，非常适合用于语音理解、多模态交互等大模型应用场景。</p>
<hr>
<h3 id="WavTokenizer-2024-08"><a href="#WavTokenizer-2024-08" class="headerlink" title="WavTokenizer (2024.08)   "></a>WavTokenizer (2024.08) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.16532"><img src="https://img.shields.io/badge/arXiv-2408.16532-b31b1b.svg?logo=arXiv" srcset="/img/loading.gif" lazyload alt="arXiv"></a> <a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavTokenizer"><img src="https://img.shields.io/badge/Github-Code-jishengpeng/WavTokenizer.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://huggingface.co/novateur/WavTokenizer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-WavTokenizer-blue.svg" srcset="/img/loading.gif" lazyload alt="model"></a></h3><p>WavTokenizer 是浙江大学联合阿里巴巴于 2024 年 8 月提出的一种面向大语言模型的 Neural Audio Codec，后被 ICLR 2025 接收。该方法以约 8000 小时多语种语音数据训练为基础，核心目标是更好地服务于多模态大模型需求。其设计强调两大核心指标：</p>
<ul>
<li><strong>更高的压缩率</strong>：支持 40 或 75 tokens/s 的极低 Token Rate，显著减少下游模型的计算开销。</li>
<li><strong>更丰富的语义表达</strong>：在主观语义评估中取得 SOTA 表现，提升了生成任务中的理解与控制能力。</li>
</ul>
<h4 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h4><ul>
<li>Encoder：基于卷积网络将输入音频编码为潜在连续特征表示$Z$</li>
<li>A single quantizer：将 $Z$ 单层量化，得到离散表示  $Z_g$</li>
<li>Decoder：将 $Z_g$ 重构成语音信号</li>
</ul>
<h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><p>WavTokenizer重构语音的UTMOS得分达到SOTA<br><img src="/image/audiocodec_wavtokenizer_result.png" srcset="/img/loading.gif" lazyload alt="WavTokenizer 效果"></p>
<h4 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h4><p>WavTokenizer 具备<strong>极低的 Token Rate（最低可达 40 tokens/s）</strong>，高度契合多模态大模型的建模需求。同时，依托大规模语音数据训练，其离散表示包含丰富的语义信息，支持语义理解与语音生成双重任务，在语音重构质量上也达到了当前最优水平。</p>
<hr>
<h3 id="Stable-Codec-2024-11"><a href="#Stable-Codec-2024-11" class="headerlink" title="Stable Codec (2024.11)   "></a>Stable Codec (2024.11) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.19842v1"><img src="https://img.shields.io/badge/arXiv-2411.19842-b31b1b.svg?logo=arXiv" srcset="/img/loading.gif" lazyload alt="arXiv"></a> <a target="_blank" rel="noopener" href="https://github.com/Stability-AI/stable-codec"><img src="https://img.shields.io/badge/Github-Code-Stability--AI/stable--codec.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/stable-codec-speech-16k"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-stable--codec--speech--16k-blue.svg" srcset="/img/loading.gif" lazyload alt="model"></a></h3><p>Stable Codec 是 Stability AI 于 2024 年 11 月发布的开源 Neural Audio Codec，专为大模型时代的语音建模需求而设计。该模型在超低码率（400bps / 700bps）下仍保持高保真度重建，为多模态大模型中的音频输入提供了极具竞争力的离散表示方案。</p>
<h4 id="模型架构-3"><a href="#模型架构-3" class="headerlink" title="模型架构"></a>模型架构</h4><ul>
<li><strong>Encoder / Decoder</strong>：基于自注意力机制的对称结构；</li>
<li><strong>Finite Scalar Quantization（FSQ）</strong>：替代传统 RVQ 的新型量化方式，采用固定范围标量量化，将连续表示压缩为离散 token。FSQ 简洁高效，推理速度快，且更适用于低码率场景；</li>
<li><strong>极低码率支持</strong>：支持 400bps、700bps 等超低比特率设置，在极限压缩下仍保持较高音质。</li>
</ul>
<p><img src="/image/audiocodec_stable_codec.png" srcset="/img/loading.gif" lazyload alt="Stable Codec 模型框架"></p>
<h4 id="FSQ-V-S-VQ"><a href="#FSQ-V-S-VQ" class="headerlink" title="FSQ V.S. VQ"></a>FSQ V.S. VQ</h4><p>这里提到了FSQ，为了有更直观的认识，这里列出二者的差异。</p>
<p>下图举例对比FSQ和VQ的差异：</p>
<p><img src="/image/audiocodec_fsq.png" srcset="/img/loading.gif" lazyload alt="FSQ V.S. VQ(https://github.com/lucidrains/vector-quantize-pytorch/blob/master/images/fsq.png)"></p>
<p>下表从不同维度详细对比FSQ和VQ的差异：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>FSQ</th>
<th>VQ</th>
</tr>
</thead>
<tbody><tr>
<td>量化方式</td>
<td>round(f(z))</td>
<td>argmin_c || z-c ||</td>
</tr>
<tr>
<td>量化单位</td>
<td>每个维度单独量化</td>
<td>整个向量最近邻</td>
</tr>
<tr>
<td>编码方式</td>
<td>标量量化（整数）</td>
<td>码本索引</td>
</tr>
<tr>
<td>并行性</td>
<td>高（可并行）</td>
<td>低（需查找）</td>
</tr>
<tr>
<td>表达粒度</td>
<td>粗，从坐标轴限制（维度独立）</td>
<td>细，全局表示能力较强（跨维度）</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>低</td>
<td>高</td>
</tr>
</tbody></table>
<p>整体而言：</p>
<ul>
<li>FSQ 是把高维输出按坐标轴切割成格子，每维分别处理；</li>
<li>VQ 是把输出投到一个高维超立方体的码本格点上，整体处理。</li>
</ul>
<h4 id="效果-3"><a href="#效果-3" class="headerlink" title="效果"></a>效果</h4><p>在MUSHRA 主观评测中，Stable Codec 的表现优于其他同类模型如 Mimi 和 SemanticCodec，其音质在主观评分中接近真实语音（Ground Truth）。<br><img src="/image/audiocodec_stable_codec_mushra.png" srcset="/img/loading.gif" lazyload alt="Stable Codec MUSHRA效果对比"></p>
<h4 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h4><p>Stable Codec 凭借其<strong>超低码率、高音质、Transformer 架构与 FSQ 创新量化方式</strong>，是语音大模型中的<strong>潜力型</strong> Audio Tokenizer 方案。其设计思路代表了 Neural Audio Codec 向更轻量、更高效、更大模型友好方向的发展趋势。</p>
<hr>
<h3 id="XY-Tokenizer-2025-07"><a href="#XY-Tokenizer-2025-07" class="headerlink" title="XY-Tokenizer (2025.07)   "></a>XY-Tokenizer (2025.07) <a target="_blank" rel="noopener" href="https://www.open-moss.com/cn/moss-ttsd/"><img src="https://img.shields.io/badge/Blog-MOSS--TTSD-blue.svg?logo=googlechrome" srcset="/img/loading.gif" lazyload alt="blog"></a> <a target="_blank" rel="noopener" href="https://github.com/OpenMOSS/MOSS-TTSD"><img src="https://img.shields.io/badge/Github-Code-OpenMOSS/MOSS--TTSD.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://huggingface.co/fnlp/XY_Tokenizer_TTSD_V0"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-XY__Tokenizer__TTSD__V0-blue.svg" srcset="/img/loading.gif" lazyload alt="model"></a></h3><p>XY-Tokenizer是复旦大学邱锡鹏团队推出的MOSS-TTSD项目中提出的离散Tokenizer。<br>实现了统一建模语音的语义和声学信息，支持低比特率（1 Kbps）与低帧率（12.5Hz），适用于高效语音生成任务。</p>
<h4 id="模型框架-1"><a href="#模型框架-1" class="headerlink" title="模型框架"></a>模型框架</h4><p>模型使用了双路 Whisper Encoder 进行语音编码，8层 RVQ 量化。<br>训练分为两个阶段，采用多任务学习策略：<br>阶段一：语义对齐 + 声学保持训练。ASR任务和重建任务，让编码器在编码语义信息的同时保留粗粒度的声学信息。<br>阶段二：生成式声学细化。固定住编码器和量化层部分，只训练解码器部分。通过重建损失和 GAN 损失，利用生成式模型的能力补充细粒度声学信息。<br><img src="/image/audiocodec_xy_tokenizer.png" srcset="/img/loading.gif" lazyload alt="XY-Tokenizer"></p>
<h4 id="效果-4"><a href="#效果-4" class="headerlink" title="效果"></a>效果</h4><ul>
<li>使用 10 万小时转录语音数据进行基础训练，确保语义编码质量；</li>
<li>引入 50 万小时无标注语音数据 进行扩展训练，有效增强模型对复杂对话语音及多场景语音的建模能力；</li>
<li>支持最长 960 秒语音输入，在超长语音建模方面表现出色。<br><img src="/image/audiocodec_xy_tokenizer_result.png" srcset="/img/loading.gif" lazyload alt="XY-Tokenizer效果"></li>
</ul>
<h4 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h4><p>XY-Tokenizer 在低比特率（1 Kbps）和低帧率（12.5 Hz）下，同时实现了高质量的语义表示和声学重建能力。其两阶段训练策略和海量多样化数据的支持，使其在真实对话音频生成与理解任务中具有较高潜力。</p>
<hr>
<h3 id="higgs-audio-v2-tokenizer-2025-07"><a href="#higgs-audio-v2-tokenizer-2025-07" class="headerlink" title="higgs-audio-v2-tokenizer (2025.07)   "></a>higgs-audio-v2-tokenizer (2025.07) <a target="_blank" rel="noopener" href="https://github.com/boson-ai/higgs-audio/blob/main/tech_blogs/TOKENIZER_BLOG.md"><img src="https://img.shields.io/badge/Blog-Github-blue.svg?logo=github" srcset="/img/loading.gif" lazyload alt="blog"></a> <a target="_blank" rel="noopener" href="https://github.com/boson-ai/higgs-audio"><img src="https://img.shields.io/badge/Github-Code-boson--ai/higgs--audio.svg?logo=github" srcset="/img/loading.gif" lazyload alt="code"></a> <a target="_blank" rel="noopener" href="https://huggingface.co/bosonai/higgs-audio-v2-tokenizer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-higgs--audio--v2--tokenizer-blue.svg" srcset="/img/loading.gif" lazyload alt="model"></a></h3><p>higgs-audio-v2-tokenizer是李沐老师创业公司BosonAI于2025年7月发布的开源Audio Codec，具备极低帧率、适合多模态大模型建模。</p>
<h4 id="模型架构-4"><a href="#模型架构-4" class="headerlink" title="模型架构"></a>模型架构</h4><ul>
<li><strong>语义-声学分离融合建模</strong>：分别对 语义信息（Semantic） 和 声学信息（Acoustic） 进行独立建模，然后在量化前进行融合，使得 token 表达同时具备语义可控性与音频细节；</li>
<li><strong>Residual Vector Quantization（RVQ）</strong>：延续了业界主流的离散化方式，保证生成质量与 token 稠密度的平衡；</li>
<li><strong>极低帧率（25fps）</strong>：极大减少了 token 数量，更适合接入 LLM 进行多模态建模和语音生成任务。</li>
</ul>
<p><img src="/image/audiocodec_higgs_audio_tokenizer.png" srcset="/img/loading.gif" lazyload alt="higgs-audio-v2-tokenizer 模型框架"></p>
<h4 id="效果-5"><a href="#效果-5" class="headerlink" title="效果"></a>效果</h4><p>语义效果评估采用了（Seed-TTS eval）作为评测集，在更低比特率（BitRate）设定下，模型在语义一致性方面可与 Mimi 相媲美，表现出良好的生成对齐能力。 </p>
<p><img src="/image/audiocodec_higgs_audio_tokenizer_semantic.png" srcset="/img/loading.gif" lazyload alt="higgs-audio-v2-tokenizer 语义效果"></p>
<p>在声学效果评估方面，采用了 STFT metric 作为评估指标。评测集由 4 个子集组成： <a target="_blank" rel="noopener" href="https://ccrma.stanford.edu/~gautham/Site/daps.html">DAPS</a>（Speech）、<a target="_blank" rel="noopener" href="https://sigsep.github.io/datasets/musdb.html">MUSDB</a>（Music）、<a target="_blank" rel="noopener" href="https://research.google.com/audioset/index.html">AudioSet</a>（Sound Event）以及 Audiophile（高保真音乐）。在多种类型音频上，模型在细节保留与主观感知质量上均达到高水平。</p>
<p><img src="/image/audiocodec_higgs_audio_tokenizer_acoustic.png" srcset="/img/loading.gif" lazyload alt="higgs-audio-v2-tokenizer 声学效果"></p>
<h4 id="小结-6"><a href="#小结-6" class="headerlink" title="小结"></a>小结</h4><p>higgs-audio-v2-tokenizer 是一款专为语音大模型设计的高效 Audio Tokenizer，其创新的语义-声学分离融合机制和极低帧率设计，显著降低了建模成本，同时保持了出色的语音还原质量。它不仅为 Higgs Audio 提供Tokenizer支持，也为语音大模型的Tokenizer提供了有价值的借鉴。</p>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="Audio-Tokenizer成为大模型的关键技术"><a href="#Audio-Tokenizer成为大模型的关键技术" class="headerlink" title="Audio Tokenizer成为大模型的关键技术"></a>Audio Tokenizer成为大模型的关键技术</h3><p>随着大模型能力的不断延展，多模态甚至全模态的大模型正在深刻变革语音技术的传统范式。语音任务正在从以模块化、定制化为主的架构，逐步转向由统一大模型驱动的端到端解决方案。</p>
<p>单点的语音能力也在向大模型方向演进，以语音识别为例，OpenAI 的 Whisper 模型在通用场景下的表现已显著优于传统的定制小模型，凭借大数据预训练和强鲁棒性，在多语言、多口音、嘈杂环境下依然具备出色性能。这类“预训练+指令微调”的语音大模型方案，正逐渐成为语音识别的新主流， 具体可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662906303">OpenAI Whisper 新一代语音技术(更新至v3-turbo)</a></p>
<p>大模型已不再局限于感知层的“识别”，而是逐步向“理解”和“生成”扩展。语音模态和大模型深度融合，也已经有很多相关工作了，虽然在端到端语音交互能力方面和传统级联方案略有逊色，但是在语音理解和生成能力方面已经远超过传统的单一能力模型，具体可以参考语音大模型概述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14831605089">语音大模型概述</a></p>
<p>尽管当前端到端语音交互模型在一些低资源场景下仍存在响应速度或精度的差距，但其多任务泛化能力和统一建模框架的优势，已在多个应用方向展现出巨大的潜力。</p>
<p>展望未来，<strong>音频模态与大模型的深度融合</strong>已成为语音领域的技术主旋律，而 <strong>Audio Tokenizer 的演进</strong>，则是这一趋势背后的关键基石。</p>
<h3 id="Audio-Tokenizer的演进趋势"><a href="#Audio-Tokenizer的演进趋势" class="headerlink" title="Audio Tokenizer的演进趋势"></a>Audio Tokenizer的演进趋势</h3><ul>
<li><strong>极致压缩、更低的TokenRate</strong>：开篇提到的挑战，语音相比文本有更长的序列，即便目前较低的tokenrate，达到25token/s，相比于文本的大概3-4token/s，仍然有6倍的差异。继续把语音压缩到更低的TokenRate，有助于语音与文本的模态融合与对齐。</li>
<li><strong>全面的信息表示</strong>：在极致压缩下，保留语义信息、副语言信息、声学信息等，才能实现后续的音频相关各项任务。</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html">Audio Tokenization: An Overview</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.10735v1">How Should We Extract Discrete Audio Tokens from Self-Supervised Models?</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vector-quantize-pytorch">Vector Quantization in PyTorch</a></li>
<li><a target="_blank" rel="noopener" href="https://drscotthawley.github.io/blog/posts/2023-06-12-RVQ.html">Residual Vector Quantization: An exploration of the heart of neural audio codecs</a></li>
</ul>
<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="print-no-link">#大模型</a>
      
        <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" class="print-no-link">#多模态</a>
      
        <a href="/tags/Tokenizer/" class="print-no-link">#Tokenizer</a>
      
        <a href="/tags/audio-codec/" class="print-no-link">#audio codec</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>大模型时代的Audio Tokenizer总结与思考</div>
      <div>http://zhaoshuaijiang.com/2025/07/25/audio-codec/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>shuaijiang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年7月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>

<div style="width:100%;display:flex;justify-content:center;margin-bottom:1.5rem"><ins class="adsbygoogle" style="display:flex;justify-content:center;max-width:845px;width:100%;height:90px" data-ad-client="ca-pub-3762972897277715" data-ad-slot="3762972897277715"></ins><script> (adsbygoogle = window.adsbygoogle || []).push({}); </script></div>

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/12/23/speech-llm/" title="语音大模型概述">
                        <span class="hidden-mobile">语音大模型概述</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量
        <span id="busuanzi_value_site_pv"></span>
        次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数
        <span id="busuanzi_value_site_uv"></span>
        人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3762972897277715" crossorigin="anonymous"></script>

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
